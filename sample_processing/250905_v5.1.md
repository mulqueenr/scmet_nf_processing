
250424 Test
updated version trying to increase efficiency, barnyard

```bash
#first need to make the output dir and the log directory for bcl-convert
outdir="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized"
mkdir -p ${outdir}
mkdir -p ${outdir}/logs

cd /data/rmulqueen/projects/kismet/ #move to project directory
git clone https://github.com/mulqueenr/scmet_nf_processing ./tools/scmet_nf_processing #pull github repo
```

# Optimized Kismet v5.1

```bash
#index is scalebio homebrew set A9
#TATAGCGCGG /Volumes/data/rmulqueen/projects/kismet/design/10x_met_design_250417.xlsx
#do rev comp, trim first 2 for i7
#run on hg38 (MDA-MB-231)

cd /data/rmulqueen/projects/kismet/ #move to project directory
outdir="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized"

nextflow /data/rmulqueen/projects/kismet/tools/scmet_nf_processing/nextflow_running/kismet_processing.groovy \
--flowcellDir /data/rmulqueen/projects/kismet/seq/250904_VH01788_122_AAHFTCNM5 \
--outname kismet_optimized \
--outdir $outdir \
--ref_index /home/rmulqueen/ref/hg38_bsbolt \
--i7_idx CCGCGCTA \
--sequencing_cycles "Y50;I8;U16;Y50" \
--max_cpus 100 -resume

```

# Regular 10x cellranger-atac (SI-NA-A5)
Run in amethyst.sif

less /home/rmulqueen/tools/cellranger-atac-2.1.0/lib/python/tenkit/sample_index.py 
SI_NA_A5 = SI_3A_A5 = SI_P03_A5 = ["ATTGGGAA", "CAGTCTGG", "GGCATACT", "TCACACTC"]

```bash
singularity shell \
--bind /data/rmulqueen/projects/kismet \
--bind /home/rmulqueen/ref/ \
--bind /home/rmulqueen:/var/log/bcl-convert \
/home/rmulqueen/singularity/amethyst.sif

source /container_src/container_bashrc

ref="/home/rmulqueen/ref/refdata-cellranger-arc-GRCh38-2020-A-2.0.0"
outdir="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized"
cd ${outdir}


echo """[Header]
FileFormatVersion,2
[BCLConvert_Settings]
CreateFastqForIndexReads,0
OverrideCycles,Y50;I8;U16;Y50
[BCLConvert_Data]
Sample_ID,index
WGS,ATTGGGAA
WGS,CAGTCTGG
WGS,GGCATACT
WGS,TCACACTC""" > 250905_kismetv51_optimized_WGS.samplesheet.csv

bcl-convert \
--bcl-input-directory /data/rmulqueen/projects/kismet/seq/250904_VH01788_122_AAHFTCNM5 \
--output-directory ${outdir}/WGS \
--sample-sheet 250905_kismetv51_optimized_WGS.samplesheet.csv --force

~/tools/cellranger-atac-2.1.0/cellranger-atac count \
--id=WGS \
--reference=${ref} \
--fastqs=${outdir}/WGS \
--sample=WGS \
--project=WGS \
--localcores=300 \
--localmem=1000

#make splitting barcode list from whitelist
outdir="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized/WGS/WGS/sc_bams"
mkdir -p $outdir
indir="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized/WGS/WGS/"
dna_barcodes="${indir}/outs/filtered_peak_bc_matrix/barcodes.tsv"
bam="${indir}/outs/possorted_bam.bam"

awk -F, 'OFS="\t" {print $1,$1}' ${dna_barcodes} > ${outdir}/cell_id.tsv

cd $outdir
#split to chunks of 500 cells for i/o purposes
split -l 500 --numeric-suffixes ${outdir}/cell_id.tsv ${outdir}/cell_id.split.

#run cell splitting for each 500 chunk
for i in cell_id.split* ; do
/home/rmulqueen/.local/bin/sinto filterbarcodes --bam ${bam} --cells $i -p 100 --barcodetag "CB" --outdir ./sc_dna_bam ;
done


#generate library complexity based on 10% downsample rates
#count unique chr:start sites
function proj_complexity() {
cellid="WGS_${1::-6}"
for i in $(seq 0.1 0.1 1.0); do
uniq_count=$(samtools view -F 3332 -s $i $1 \
| awk 'OFS="\t"{print $3,$4}' \
| sort \
| uniq -c \
| wc -l)
total_count=$(samtools view -F 3332 -s $i $1 | wc -l)
echo "${cellid},${i},${total_count},${uniq_count}"; done > ${cellid}.projected_metrics.txt
}

export -f proj_complexity
parallel -j 100 proj_complexity ::: $(ls *-1.bam)

#excluding reads that meet any below conditions:
#read unmapped (0x4)
#not primary alignment (0x100)
#read is PCR or optical duplicate (0x400)
#supplementary alignment (0x800)

```


Project complexity
```R
library(ggplot2)
library(patchwork)
library(drc)
library(parallel)
library(dplyr)

#project complexity for cells passing QC
dat_kismet<-do.call("rbind",lapply(list.files(path="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized/reports/projected_size",pattern="*projected_metrics.txt",full.names=T),function(x) read.table(x,header=F,sep=",")))
dat_kismet$method<-"kismet"

dat_wgs<-do.call("rbind",lapply(list.files(path="/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized/WGS/WGS/sc_bams/sc_dna_bam/",pattern="*projected_metrics.txt",full.names=T),function(x) read.table(x,header=F,sep=",")))
dat_wgs$method<-"wgs"

dat<-rbind(dat_kismet,dat_wgs)

colnames(dat)<-c("sample","downsample_perc","total_frag","uniq_frag","method")

#filter to kismet top 1000 cells
top_cells<- as.data.frame(dat %>% filter(as.numeric(downsample_perc)==1)  %>% slice_max(n=1000,by=method,order_by=uniq_frag))
top_cells<- top_cells$sample

dat<-dat[dat$sample %in% top_cells,]


michaelis_menten_fit<-function(x){
    mm<-dat[dat$sample==x,]
    colnames(mm)<-c("cellid","downsample_perc","S","v","method")
    model.drm <- drm(as.numeric(v) ~ as.numeric(S), data = mm, fct = MM.2())
    km_uniq <- data.frame(S = coef(model.drm)[2])
    km_uniq$v <- predict(model.drm, newdata = km_uniq)
    vmax<-as.numeric(coef(model.drm)[1])
    km<-as.numeric(coef(model.drm)[2])
    current_total_reads<-as.numeric(mm[mm$downsample_perc==1.0,]$S)
    current_uniq_reads<-as.numeric(mm[mm$downsample_perc==1.0,]$v)
    method<-mm[mm$downsample_perc==1.0,]$method
    return(c(x,vmax,km,km_uniq$v,current_total_reads,current_uniq_reads,method))
}

projdat<-as.data.frame(do.call("rbind",mclapply(mc.cores=100,unique(dat$sample),michaelis_menten_fit)))


colnames(projdat)<-c("sample",
"projected_saturated_fragments",
"projected_optimal_seq_effort",
"projected_reads_at_optimal_effort",
"current_total_reads",
"current_uniq_reads",
"method")


projdat$projected_saturated_fragments<-as.numeric(projdat$projected_saturated_fragments)
projdat$current_total_reads<-as.numeric(projdat$current_total_reads)
projdat$projected_reads_at_optimal_effort<-as.numeric(projdat$projected_reads_at_optimal_effort)

plt1<-ggplot(projdat,aes(x=method,y=log10(projected_reads_at_optimal_effort*2),alpha=0.5,color=method))+
geom_jitter()+
theme_minimal()+
ylim(c(0,7))+
ggtitle(paste0("50% Saturated Library Read Counts:\n","Kismet Mean: ",
round(mean(projdat[projdat$method=="kismet",]$projected_reads_at_optimal_effort)*2),
"\nWGS Mean: ",
round(mean(projdat[projdat$method=="wgs",]$projected_reads_at_optimal_effort)*2)))
ggsave(plt1,file="uniq_reads_per_method.pdf")

plt2<-ggplot(projdat,aes(x=current_total_reads,y=projected_reads_at_optimal_effort,color=method))+geom_point()+theme_minimal()
ggsave(plt2,file="uniq_reads_per_method2.pdf")

projdat %>% group_by(method) %>% summarize(count=n(),reads=mean(projected_reads_at_optimal_effort))


```

Merge data with scalebio dcis project cell lines to check correlation

#run per line 
```bash
singularity shell \
--bind /data/rmulqueen/projects/scalebio_dcis \
--bind /data/rmulqueen/projects/kismet \
~/singularity/amethyst.sif
```

```R
source("/data/rmulqueen/projects/scalebio_dcis/tools/scalemet_dcis/src/amethyst_custom_functions.R") #to load in
setwd("/data/rmulqueen/projects/kismet/data/250905_kismetv51_optimized/")
scale_celllines<-readRDS(file="/data/rmulqueen/projects/scalebio_dcis/data/250815_milestone_v1/merged_data/01_celllines.amethyst.rds")
scale_celllines@metadata$method="scale"
summary(scale_celllines@metadata$cg_cov/scale_celllines@metadata$unique_reads)
#150bp PE
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.3198  0.6718  0.6893  0.6853  0.7037  0.7645 

kismet<-readRDS("kismet_optimized.amethyst.rds")
kismet@metadata$unique_reads<-kismet@metadata$READ_PAIRS_EXAMINED
kismet<-subsetObject(
    kismet,
    cells=row.names(kismet@metadata[kismet@metadata$unique_reads>quantile(kismet@metadata$unique_reads,0.75),])
    ) #number set by 3rd Q of kismet

summary(kismet@metadata$cg_cov/kismet@metadata$READ_PAIRS_EXAMINED)
#50bp PE
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.1455  0.1867  0.1952  0.1956  0.2034  0.3653 

kismet@metadata$method="kismet"
kismet@metadata$sample="MDA-MB-231"


col_reduced<-intersect(colnames(scale_celllines@metadata ),colnames(kismet@metadata))

scale_celllines@metadata <- scale_celllines@metadata[,col_reduced]

kismet@metadata <- kismet@metadata[,col_reduced]

dat <- combineObject(objList = c(scale_celllines,kismet),genomeMatrices="cg_100k_score")

#rerun to ensure same calculation
dat@genomeMatrices[["cg_1mb_percent"]] <- makeWindows(dat, 
                                                    stepsize=1000000,
                                                     type = "CG", 
                                                     metric = "percent", 
                                                     threads = 200, 
                                                     index = "chr_cg", 
                                                     nmin = 2) 

#cluster on large windows
window_name="cg_1mb_percent"
metric="percent"
threads=100
neighbors=50
est_dim=10
k_pheno=15
dist=0.1
obj<-dat
outname="kismet_scale_cluster"

#filter windows by cell coverage
obj@genomeMatrices[[window_name]] <- obj@genomeMatrices[[window_name]][rowSums(!is.na(obj@genomeMatrices[[window_name]])) >= 45, ]

print("Running IRLBA reduction...")
obj@reductions[[paste(window_name,"irlba",sep="_")]] <- runIrlba(obj, genomeMatrices = c(window_name), dims = est_dim, replaceNA = c(0))

print("Clustering on coverage regressed reduction...")
obj <- runCluster(obj, k_phenograph = k_pheno, reduction = paste(window_name,"irlba",sep="_")) 
# consider increasing k_phenograph to 50 for larger datasets

obj <- runUmap(obj, neighbors = neighbors, dist = dist, method = "euclidean", reduction = paste(window_name,"irlba",sep="_")) 

print("Plotting...")
p1 <- dimFeature(obj, colorBy = sample, reduction = "umap") + ggtitle(paste(window_name,"Clusters"))
p2 <- dimFeature(obj, colorBy = method, reduction = "umap") + ggtitle(paste(window_name,"Samples"))
p3 <- dimFeature(obj, colorBy = log10(cov), pointSize = 1) + scale_color_gradientn(colors = c("black", "turquoise", "gold", "red"),guide="colourbar") + ggtitle("Coverage distribution")
p4 <- dimFeature(obj, colorBy = mcg_pct, pointSize = 1) + scale_color_gradientn(colors = c("black", "turquoise", "gold", "red")) + ggtitle("Global %mCG distribution")
plt<-plot_grid(p1, p2,p3, p4,ncol=2)
ggsave(plt,file=paste0(outname,"_umap.pdf"),width=20,height=20)  

#doesnt integrate very well
#merge cells and do correlation over windows
obj@metadata$cell_method<-paste(obj@metadata$sample,obj@metadata$method,sep="_")

table(obj@metadata$cell_method)

obj_smoothed <- calcSmoothedWindows(obj, 
                            type = "CG", 
                            threads = 200,
                            step = 50000,
                            smooth = 1,
                            index = "chr_cg",
                            groupBy = "cell_method", 
                            returnSumMatrix = TRUE, # save sum matrix for DMR analysis
                            returnPctMatrix = TRUE)

#pctmatrix is actually scores and not just straight percentage
obj_smoothed$pct_matrix[["MDA-MB-231_kismet"]]<-obj_smoothed$sum_matrix[["MDA-MB-231_kismet_c"]]/obj_smoothed$sum_matrix[["MDA-MB-231_kismet_t"]]
obj@genomeMatrices[["50kb_pct"]]<-obj_smoothed$pct_matrix
#correlate across columns of PctMatrix
cor_mat<-cor(as.data.frame(obj_smoothed$pct_matrix)[,c(4:7)],use="pairwise.complete",method="spearman")
library(ComplexHeatmap)
pdf("cellline_cor_50kbp.pdf")
Heatmap(as.matrix(cor_mat))
dev.off()

bigwig_output<-function(
    obj,
    tracks="cg_cluster_tracks"){
    for(i in 4:ncol(obj@genomeMatrices[[tracks]])){
        cluster=names(obj@genomeMatrices[[tracks]])[i]
        out_bw<-as.data.frame(obj@genomeMatrices[[tracks]])
        out_bw<-out_bw[c("chr","start","end",cluster)]
        out_bw<-GRanges(out_bw[complete.cases(out_bw),])
        names(out_bw@elementMetadata)<-"score"
        out_bw<-out_bw[unique(findOverlaps(out_bw, type = "any", select = "first"))]
        out_bw <- resize(out_bw, width=1000, fix='start') #resize to avoid 1base overlap
        genome(out_bw)<-"hg38"
        hg38_seq_info<-Seqinfo(genome="hg38")
        seqlengths(out_bw)<-as.data.frame(hg38_seq_info)[hg38_seq_info@seqnames %in% out_bw@seqnames,]$seqlengths
        print(paste("Saving bigwig for...",cluster))
        export(out_bw,con=paste(tracks,cluster,"bw",sep="."),format='bigWig')}
}

bigwig_output(obj,tracks="50kb_pct")

```